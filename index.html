<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Propagation of Epidemics in Citation Networks</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Propagation of Epidemics in Citation Networks</h1>
      <h2 class="project-tagline">CS-8803 Data Science for Epidemiology Project</h2>
      <a href="https://github.com/rohitmujumdar/Propagation-of-Epidemics-in-Citation-Networks/" class="btn">View on GitHub</a>
    </section>

    <section class="main-content">
      
<a id="authors-and-contributors" class="anchor" href="#authors-and-contributors" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Team</h2>
<p>David Kartchner &sdot; Rohit Mujumdar </p>

<p><center><img src="david_kartchner.png" style="width:700px; height: 340px; " alt="something"></center></p>
<p><center><img src="IMG_20191225_134848.jpg" style="width:700px; height: 250px; " alt="something"></center></p>

<h2>
<a id="Project Summary" class="anchor" href="#Introduction" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Introduction</h2>
<p>Academic research should live or die by its own merits. However, human cognitive shortcuts have long believed to give undue advantage to particular institutions or researchers, sometimes blinding reviewers to errors of lack of rigor on their work. We investigate this imbalance by studying the spread of ideas across academic research networks using disease spread models adapted from epidemiology. We specifically focus on spread of ideas in the domain of Machine Learning (ML), a specialised area of research in Computer Science. We take the papers published in the year 2014 as our set of base “pathogens” and assess the network growth dynamics of idea spread amongst major ML conferences. We assess if ideas of the same calibre spread faster when originating from more prestigious institutions. We define data-centric measures for institutional prestige and idea quality and use both network-based simulation and statistical estimation to quantify how these factors affect idea propagation.</p>


<h2>
<a id="Approach" class="anchor" href="#Approach" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Approach</h2>
<p>We approach Content Moderation as a binary classification problem with 2 broad categories - offensive images and non-offensive images. </p>
<p>We use Convolutional Neural Networks (CNNs) to perform a supervised binary classification of images into these two categories. Specifically, we perform feature extraction on the images in our dataset using transfer learning from the following pre-trained CNNs to build the binary classifier.
</p>

<h2>
<a id="Results" class="anchor" href="#Results" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Results</h2>
<p>We trained our dataset using the four different classifiers mentioned above, all of them CNN architectures. </p>
<p>We split the data set into 3 sets - train, validation and test. The classifiers are trained using a transfer learning approach where we extract features from pre-trained convnets and modify the final fully-connected layer to suit our data. The transfer learning is performed on  a ConvNet pretrained on ImageNet and backprop is done on only the final, modified layer and the weights from the pretrained layers are kept as is. Thus the rest of the ConvNet as a fixed feature extractor for the new dataset. </p>
<p>Details about the data used: (Images are sampled from the videos from <a href="#ref2">[4]</a> via an offline process)</p>
<table>
  
<h2>
<a id="Resources" class="anchor" href="#Resources" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Resources</h2>
<table>
  
  <tr>
    <th>Type of Data<br></th>
    <th>Number of Violent Images</th>
    <th>Number of Non-Violent Images</th>
    <th>Total Number of Images</th>
  </tr>
  <tr>
    <td>Training</td>
    <td>8091</td>
    <td>9409</td>
    <td>17500</td>
  </tr>
  <tr>
    <td>Validation</td>
    <td>1513</td>
    <td>1506</td>
    <td>3019</td>
  </tr>
  <tr>
    <td>Test</td>
    <td>1506</td>
    <td>1501</td>
    <td>3007</td>
  </tr>
</table>

<table>
  <tr>
    <th>Type of Data<br></th>
    <th>Number of Violent Images</th>
    <th>Number of Non-Violent Images</th>
    <th>Total Number of Images</th>
  </tr>
  <tr>
    <td>Training</td>
    <td>8091</td>
    <td>9409</td>
    <td>17500</td>
  </tr>
  <tr>
    <td>Validation</td>
    <td>1513</td>
    <td>1506</td>
    <td>3019</td>
  </tr>
  <tr>
    <td>Test</td>
    <td>1506</td>
    <td>1501</td>
    <td>3007</td>
  </tr>
</table>
<p>As per our dataset, a ‘dumb’ classifier (always ‘yes’ or always ‘no’)  or a random classifier(randomly classification) would perform classification with an accuracy of close to 50%.</p>

<h4>Algorithms used and their parameters</h4>
<table>
  <tr>
    <th>Parameter</th>
    <th>Value used</th>
    <th>Motivation</th>
  </tr>
  <tr>
    <td>num_classes</td>
    <td>2</td>
    <td>The images need to be classified as ‘violent’ or ‘non-violent’.</td>
  </tr>
  <tr>
    <td>batch_size</td>
    <td>4/8</td>
    <td>Batch-size was 8 for Inception, AlexNet and VGG. ResNet used a batch size of 4. The batch sizes was chosen due to computational limitations of the machines we were using.</td>
  </tr>
  <tr>
    <td>num_epochs</td>
    <td>15</td>
    <td>We used the pre-trained models as feature extractors and were training the last layer for our classification problem. In this case, 15 epochs was sufficient for the models to learn the problem. We were not noticing any changes beyond the first few epochs, so we capped it at 15.</td>
  </tr>
  <tr>
    <td>learningRate, momentum</td>
    <td>0.001, 0.9 respectively</td>
    <td>The learning rate and momentum were set to these values to increase the probability of not getting stuck at a local optima.</td>
  </tr>
  <tr>
    <td>Image Transforms</td>
    <td>Resized Crop and Horizontal Flip</td>
    <td>We did this to augment the dataset to be invariant to scale and rotation changes. This captures the variety/variance of real world scenes that will be encountered by the system during the test phase.</td>
  </tr>
  <tr>
    <td>Validation</td>
    <td></td>
    <td>We had a validation dataset (that was independent from both the training and the testing data) that was used to determine if the model was overfitting. The best performing model on the validation data for a particular network was used as the final network for testing.</td>
  </tr>
</table>
<h4>Results</h4>
<center>
<table style="align-self: center; ">
  <tr>
    <th><span style="font-weight:700">Model</span></th>
    <th><span style="font-weight:700">Accuracy</span></th>
    <th><span style="font-weight:700">Precision</span></th>
    <th><span style="font-weight:700">F1 Score</span></th>
    <th><span style="font-weight:700">Recall</span></th>
  </tr>
  <tr>
    <td>AlexNet</td>
    <td>89%</td>
    <td>90%</td>
    <td>88</td>
    <td>86%</td>
  </tr>
  <tr>
    <td>VGG</td>
    <td>92%</td>
    <td>95%</td>
    <td>92</td>
    <td>89%</td>
  </tr>
  <tr>
    <td>InceptionV3</td>
    <td>89%</td>
    <td>92%</td>
    <td>89</td>
    <td>86%</td>
  </tr>
  <tr>
    <td>Resnet</td>
    <td>88%</td>
    <td>88%</td>
    <td>88</td>
    <td>88%</td>
  </tr>
</table>
</center>
<h2>
<a id="Qualitative Results and trends" class="anchor" href="#Qualitative Results and trends" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Qualitative Results and trends</h2>
<h4>
  <strong>
  <a id="datasets" class="anchor" href="#datasets" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>True Positives (Violent classified as violent)
</strong>
</h4>
<p>
  As we can see from the examples below, these are images in which violence is very apparent in each frame. Each image has two or more people engaged in a physical fight. The fight is not occluded and therefore, our model is able to classify it accurately.
</p>

<table align="center">
  <tr>
    <td><center><img src="truepositive1.jpg" style="width:300px; height: 240px; " alt="something"></center></td>
    <td><center><img src="truepositive2.jpg" style="width:300px; height: 240px; " alt="something"></center></td>
  </tr>
</table>
<h4>
  <strong>
  <a class="anchor" href="#datasets" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>True Negatives (Non-violent classified as non-violent)
</strong>
</h4>
<p>
  These are examples of different types of non-violent images our model might encounter in the real world. Scenes from movies and news clips. People are engaged in a conversation or cheering for their favourite sports teams. This proves that our model does not simply classify images with people in close proximity as violent and thus proves that the model has learned useful characteristics in differentiating violent and non-violent images.
</p>
<table>
  <tr>
    <td><center><img src="truenegative2.jpg" style="width:700px; height: 270px; " alt="something"></center></td>
    <td><center><img src="truenegative3.jpg" style="width:700px; height: 270px; " alt="something"></center></td>
    <td><center><img src="truenegative1.jpg" style="width:700px; height: 270px; " alt="something"></center></td>
  </tr>
</table>
<h4>
  <strong>
  <a  class="anchor" href="#datasets" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>False Negatives (Violent classified as non-violent)
</strong>
</h4>
<p>
  There were two major categories within the set of false negative images.
  <ol>
    <li>
      Since we extracted frames from a video, there are certain images which are non-violent but have been labelled as ‘violent’ in the ground truth. These images increase the false negative rate in our model. One of the post-processing steps that we could consider would be to use a majority voting decision for all frames of the video, for our particular dataset.
    </li>
  </ol>

  <table>
  <tr>
    <td><center><img src="falsenegative1.jpg" style="width:600px; height: 200px; " alt="something"></center></td>
    <td><center><img src="falsenegative2.jpg" style="width:600px; height: 200px; " alt="something"></center></td>
  </tr>
  <tr>
    <td><center><img src="falsenegative3.jpg" style="width:600px; height: 200px; " alt="something"></center></td>
    <td><center><img src="falsenegative4.jpg" style="width:600px; height: 200px; " alt="something"></center></td>
  </tr>
</table>

<br>
  <ol>
    <li>
      Another category of false negatives arise from the presence of occlusions or when images in themselves are not violent but as a succession of images demonstrates a violent intention.
    </li>
  </ol>
<br>
  <table>
  <tr>
    <td><center><img src="occlusion3.jpg" style="width:400px; height: 220px; " alt="something"></center></td>
    <td><center><img src="occlusion2.jpg" style="width:400px; height: 220px; " alt="something"></center></td>
  </tr>
  <tr>
    <td><center><img src="occlusion.jpg"  style="width:400px; height: 220px; " alt="something"></center></td>
    <td><center><img src="occlusion4.jpg" style="width:400px; height: 220px; " alt="something"></center></td>
  </tr>
</table>

  <p>
    For example, in the first row of images, due to occlusion and blurring the model is unable to identify it as a violent image.
    The second row shows cases of violence where a particular image in itself may not contain enough characteristics to be called as violent but it is true that the series of images demonstrates violence or that there is potential violence that could follow (for eg., based on the setting in the image on the bottom right, we might be able to extrapolate that there was violence demonstrated at some later point in time)

  </p>
</p>

<h4>
  <strong>
  <a  class="anchor" href="#datasets" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>False Positives ( Non violent classified as violent)
</strong>
</h4>
<p>
  We noticed a few interesting set of cases for false positive images.
</p>
<p>
  In the first row, we suspect that the presence of a crowd in the left-most image is leading to it being classified as violent. Similarly, the positioning of the players in the right-most image seem similar to the common setting in most violent scenes.
</p>
<p>
  The images in the bottom row were interesting candidates of false positives. We would need to analyze the salient regions using grad-cam visualizations in phase 2 to see what aspect of the image could be causing the misclassification. These are the kind of incorrect classifications which could be easily overturned by a human moderator (or for users, help them understand better and raise an appeal to overturn moderation decisions) if they were able to see what regions were contributing to the misclassification. Further, we believe this should help us train the model better by providing better examples that allow the model to learn better.
</p>

<table>
  <tr>
    <td><center><img src="falsepositive1.jpg"  alt="something"></center></td>
    <td><center><img src="falsepositive2.jpg"  alt="something"></center></td>
    <td><center><img src="falsepositive3.jpg"  alt="something"></center></td>
  </tr>
  <tr>
    <td><center><img src="falsepositive4.jpg"  alt="something"></center></td>
    <td><center><img src="falsepositive5.jpg"  alt="something"></center></td>
    <td><center><img src="falsepositive6.jpg"  alt="something"></center></td>
  </tr>
</table>


<h2>
<a id="Conclusion and future work" class="anchor" href="#Conclusion and future work:" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Conclusion and future work</h2>
<p>Accuracies above 88% were observed using the proposed approach on all 4 models, with a highest accuracy of 95% and highest recall of 89%. Qualitative results demonstrate cases where the models are able to correctly classify a range of violent images as violent as well as specific cases where the classifier has not been able to detect violence correctly with potential explanations for the same.
</p>
<p>
  In phase 2, we intend to integrate Grad-CAM to visualize and explain the results to complete what would make for a minimum viable robust, explainable content moderation system.
</p>
<p>
  In addition to the above, we intend to explore if we can automatically replace salient regions identified in violent images with a blurred version of the region as a means to potentially auto-moderate the image.
</p>
<p>
  We also intend to test our model with varying degrees of violence (eg. replacing two people fighting with an entire mob) to note its performance and investigate the salient regions that get returned for images classified as offensive.
</p>
<h2>
<a id="authors-and-contributors" class="anchor" href="#authors-and-contributors" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Team</h2>
<p>Shalini Chaudhuri &sdot; Rohit Mujumdar &sdot; Sushmita Singh &sdot; Sreehari Sreejith</p>

      <footer class="site-footer">
        <span class="site-footer-credits">
          <h2>References</h2>
          <ol>
            <li id="ref1">
              R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh and D. Batra, "Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization," 2017 IEEE International Conference on Computer Vision (ICCV), Venice, 2017, pp. 618-626.
            </li>
            <li id="ref2" href="https://www.eff.org/deeplinks/2019/04/content-moderation-broken-let-us-count-ways">
              https://www.eff.org/deeplinks/2019/04/content-moderation-broken-let-us-count-ways
            </li>
            <li id="ref3">
              C.H. Demarty,C. Penet, M. Soleymani, G. Gravier. VSD, a public dataset for the detection of violent scenes in movies: design, annotation, analysis and evaluation. In Multimedia Tools and Applications, May 2014.
            </li>
            <li id="ref4" href="https://www.kaggle.com/mohamedmustafa/real-life-violence-situations-dataset">
              https://www.kaggle.com/mohamedmustafa/real-life-violence-situations-dataset
            </li>
            <li id="ref5" href="https://www.interdigital.com/data_sets/violent-scenes-dataset#">
              https://www.interdigital.com/data_sets/violent-scenes-dataset# 
            </li>
          </ol>
        </span>
      </footer>

    </section>

  
  </body>
</html>
